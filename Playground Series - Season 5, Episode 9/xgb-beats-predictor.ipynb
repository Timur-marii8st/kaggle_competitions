{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91720,"databundleVersionId":13345277,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import seaborn as sns\nimport numpy as np\nimport polars as pl\nimport optuna\nfrom xgboost import XGBRegressor\nfrom xgboost.callback import EarlyStopping\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\n\n\nfrom itertools import combinations\nfrom typing import List","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:54:31.124510Z","iopub.execute_input":"2025-09-04T15:54:31.124669Z","iopub.status.idle":"2025-09-04T15:54:37.130092Z","shell.execute_reply.started":"2025-09-04T15:54:31.124649Z","shell.execute_reply":"2025-09-04T15:54:37.129431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pl.read_csv('/kaggle/input/playground-series-s5e9/train.csv')\ntest_df = pl.read_csv('/kaggle/input/playground-series-s5e9/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:54:37.131696Z","iopub.execute_input":"2025-09-04T15:54:37.132326Z","iopub.status.idle":"2025-09-04T15:54:39.137478Z","shell.execute_reply.started":"2025-09-04T15:54:37.132281Z","shell.execute_reply":"2025-09-04T15:54:39.136780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# looking ad dataset(df)\ntrain_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:54:39.138143Z","iopub.execute_input":"2025-09-04T15:54:39.138407Z","iopub.status.idle":"2025-09-04T15:54:39.175731Z","shell.execute_reply.started":"2025-09-04T15:54:39.138387Z","shell.execute_reply":"2025-09-04T15:54:39.174874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ITS NOT FOR LOGGING, ITS FOR LORAPHMING THE BAD COLUMNS!\ndef loging_df(df: pl.DataFrame, bad_cols: List) -> pl.DataFrame:\n    for col in bad_cols:\n        df = df.with_columns(pl.col(col).log1p())\n\n    return df\n\n# Cut off the tails with extreme outliners\ndef windsorizing_df(df: pl.DataFrame, bad_cols: List, low=0.01, high=0.99) -> pl.DataFrame:\n    for col in bad_cols:\n        lo = df[col].quantile(low)\n        hi = df[col].quantile(high)\n        \n        df = df.with_columns(\n            pl.col(col).clip(lower_bound=lo, upper_bound=hi).alias(col)\n        )\n    \n    return df\n\n# scaling with x_scaled = (x - median(X))/IQR(X)\ndef robust_scaling(df: pl.DataFrame, bad_cols: List) -> pl.DataFrame:\n    scaler = RobustScaler()\n    \n    for col in bad_cols:\n        values = df[col].to_numpy().reshape(-1, 1)\n        \n        scaled = scaler.fit_transform(values).flatten()\n        df = df.with_columns(\n            pl.Series(name=col, values=scaled)\n        )\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:54:39.176550Z","iopub.execute_input":"2025-09-04T15:54:39.176752Z","iopub.status.idle":"2025-09-04T15:54:39.335649Z","shell.execute_reply.started":"2025-09-04T15:54:39.176737Z","shell.execute_reply":"2025-09-04T15:54:39.334178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bad_cols = ['RhythmScore', 'AudioLoudness', 'VocalContent', 'AcousticQuality', 'InstrumentalScore', 'LivePerformanceLikelihood', 'MoodScore']\n\ntrain_df = loging_df(train_df, bad_cols)\ntrain_df = windsorizing_df(train_df, bad_cols)\ntrain_df = robust_scaling(train_df, bad_cols)\n\ntest_df = loging_df(test_df, bad_cols)\ntest_df = windsorizing_df(test_df, bad_cols)\ntest_df = robust_scaling(test_df, bad_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_to_combine = ['RhythmScore', 'AudioLoudness', 'AcousticQuality', 'MoodScore']\n\n# Making new features\nfor col1, col2 in combinations(features_to_combine, 2):\n    train_df = train_df.with_columns(\n        (pl.col(col1) * pl.col(col2)).alias(f'{col1}_x_{col2}'),\n        (pl.col(col1) / (pl.col(col2) + 1e-6)).alias(f'{col1}_div_{col2}')\n    )\n    test_df = test_df.with_columns(\n        (pl.col(col1) * pl.col(col2)).alias(f'{col1}_x_{col2}'),\n        (pl.col(col1) / (pl.col(col2) + 1e-6)).alias(f'{col1}_div_{col2}')\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = train_df.drop([\"id\", \"BeatsPerMinute\"])\ny = train_df[\"BeatsPerMinute\"]\nX_test = test_df.drop(\"id\")\n\nFOLDS = 20\nFEATURES = X.columns\n\n# KFold setup\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n# Arrays to store predictions\noof = np.zeros(len(train_df))\npred = np.zeros(len(test_df))\n\nX_np = X.to_numpy()\ny_np = y.to_numpy()\nX_test_np = X_test.to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:54:39.338474Z","iopub.status.idle":"2025-09-04T15:54:39.338742Z","shell.execute_reply.started":"2025-09-04T15:54:39.338603Z","shell.execute_reply":"2025-09-04T15:54:39.338617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lets optune our parameters\ndef objective(trial):\n    params = {\n        \"device\": \"cpu\",\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 3000),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 20.0),\n        \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 0, 10),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 10.0),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 10.0),\n        \"eval_metric\": \"rmse\",\n        \"enable_categorical\": True,\n        \"tree_method\": \"hist\",\n        \"random_state\": 42\n    }\n    \n    inner_kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    inner_rmses = []\n    \n    for train_idx, val_idx in inner_kf.split(X_np):\n        x_tr, y_tr = X_np[train_idx], y_np[train_idx]\n        x_val, y_val = X_np[val_idx], y_np[val_idx]\n        \n        early_stopping = EarlyStopping(\n            rounds=100,\n            min_delta=1e-5,\n            save_best=True\n        )\n        \n        model = XGBRegressor(**params, callbacks=[early_stopping])\n        model.fit(\n            x_tr, y_tr,\n            eval_set=[(x_val, y_val)],\n            verbose=False\n        )\n        \n        val_pred = model.predict(x_val)\n        rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n        inner_rmses.append(rmse)\n    \n    return np.mean(inner_rmses)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=50)\n\nbest_params = study.best_params\nbest_params.update({\n    \"device\": \"cpu\",\n    \"eval_metric\": \"rmse\",\n    \"enable_categorical\": True,\n    \"tree_method\": \"hist\",\n    \"random_state\": 42\n})\n\nprint(\"Best params from Optuna:\", best_params)\n\nfor i, (train_indices, valid_indices) in enumerate(kf.split(X_np, y_np)):\n    print(f\"\\n{'#'*5} Fold {i+1} {'#'*5}\")\n    x_train = X_np[train_indices]\n    y_train = y_np[train_indices]\n    x_valid = X_np[valid_indices]\n    y_valid = y_np[valid_indices]\n    early_stopping = EarlyStopping(\n        rounds=100,\n        min_delta=1e-5,\n        save_best=True\n    )\n    model = XGBRegressor(**best_params, callbacks=[early_stopping])\n   \n    model.fit(\n        x_train, y_train,\n        eval_set=[(x_valid, y_valid)],\n        verbose=100\n    )\n   \n    oof[valid_indices] = model.predict(x_valid)\n    pred += model.predict(X_test_np)\n   \n    rmse = np.sqrt(mean_squared_error(y_valid, oof[valid_indices]))\n    print(f\"Fold {i+1} RMSE: {rmse:.4f}\")\n   \npred /= FOLDS\n       \nfull_rmse = np.sqrt(mean_squared_error(y_np, oof))\nprint(f\"\\nFinal CV RMSE: {full_rmse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:54:39.339328Z","iopub.status.idle":"2025-09-04T15:54:39.339565Z","shell.execute_reply.started":"2025-09-04T15:54:39.339452Z","shell.execute_reply":"2025-09-04T15:54:39.339463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pl.read_csv(\"/kaggle/input/playground-series-s5e9/sample_submission.csv\")\n\nprint('predict mean :',pred.mean())\nprint('predict median :',np.median(pred))\n\ny_pred_after = np.clip(pred, 46.718, 206.037)\n\nsubmission = submission.with_columns(\n    pl.Series(name=\"BeatsPerMinute\", values=y_pred_after)\n)\n\nsubmission.write_csv(\"submission.csv\")\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:54:39.340728Z","iopub.status.idle":"2025-09-04T15:54:39.340954Z","shell.execute_reply.started":"2025-09-04T15:54:39.340838Z","shell.execute_reply":"2025-09-04T15:54:39.340849Z"}},"outputs":[],"execution_count":null}]}
