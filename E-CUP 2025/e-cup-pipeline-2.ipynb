{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12832321,"sourceType":"datasetVersion","datasetId":8115585}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport math\nimport gc\nimport sys\nimport warnings\nimport subprocess\nimport random\nfrom typing import List, Tuple, Dict, Optional, Set\nfrom datetime import timedelta, datetime\nfrom collections import OrderedDict\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport threading\n\nos.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\nos.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\nos.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\nos.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n\nimport numpy as np\nimport polars as pl\nimport scipy.sparse as sp\n\ndef ensure_pkg(pkg, import_name=None, version=None):\n    try:\n        __import__(import_name or pkg)\n    except Exception:\n        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg + (f\"=={version}\" if version else \"\")]\n        print(\"Installing:\", \" \".join(cmd))\n        subprocess.check_call(cmd)\n\nensure_pkg(\"implicit\", \"implicit\")\nensure_pkg(\"annoy\", \"annoy\")\n\ntry:\n    from catboost import CatBoostRanker, Pool\n    CATBOOST_AVAILABLE = True\nexcept Exception:\n    CATBOOST_AVAILABLE = False\n    warnings.warn(\"CatBoost не найден. Реранкинг отключён.\")\n\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom implicit.nearest_neighbours import bm25_weight, BM25Recommender\nfrom implicit.als import AlternatingLeastSquares\nfrom annoy import AnnoyIndex\n\n# -----------------------\n# 1) Config\n# -----------------------\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)\nCPU_COUNT = os.cpu_count() or 4\nNUM_THREADS = min(8, CPU_COUNT)\n\nTMP_DIR = \"./tmp_stream\"\nos.makedirs(TMP_DIR, exist_ok=True)\n\nMIN_DELIVERED_ORDERS = 3\nTRACKER_SAMPLE_FRAC = 0.4\n\nTRAIN_WINDOW_DAYS = 120\n\n# ALS\nALS_FACTORS = 16\nALS_ITERS = 8\nALS_REG = 0.01\nALS_REFRESH_EVERY = 0\n\n# Candidate sizes\nALS_TOPN = 150\nI2I_TOPN = 200\nEMB_TOPN = 150\nBLEND_TOPK = 100 \nRECENT_K_FOR_I2I = 20\n\n# Split\nVAL_DAYS = 30\nMAX_VAL_USERS = 3000\nTRAIN_CANDS_PER_USER = 100\n\n# CatBoost\nCB_ITER = 120\nCB_OD_WAIT = 40\nCB_LR = 0.08\nCB_DEPTH = 6\n\n# Weights\ntrack_w = {\"page_view\": 2.0, \"view_description\": 3.0, \"to_cart\": 1.0, \"unfavorite\": 0.0, \"favorite\": 5.0, \"remove\": 0.0, \"review_view\": 0.5}\norders_w = {\"delivered_orders\": 5.0, \"proccesed_orders\": 4.0, \"canceled_orders\": 1.5}\n\n# Embeddings\nANNOY_TREES = 20","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 2) Поиск партов\n# -----------------------\ndef find_parts(base: str) -> List[str]:\n    if os.path.isdir(base):\n        pats = [os.path.join(base, \"part-*.snappy.parquet\"),\n                os.path.join(base, \"part-*.parquet\"),\n                os.path.join(base, \"part-*\")]\n        out = []\n        for p in pats:\n            out.extend(glob.glob(p))\n        return sorted(set(out))\n    elif os.path.isfile(base):\n        return [base]\n    return []\n\nroot = \"/kaggle/input/testststs\"\ntest_path  = f\"{root}/ml_ozon_recsys_test.snappy.parquet\"\ncatal_dir  = f\"{root}/ml_ozon_recsys_train_final_categories_tree/ml_ozon_recsys_train_final_categories_tree\"\ntracker_dir= f\"{root}/ml_ozon_recsys_train_final_apparel_tracker_data/ml_ozon_recsys_train_final_apparel_tracker_data\"\norders_dir = f\"{root}/ml_ozon_recsys_train_final_apparel_orders_data/ml_ozon_recsys_train_final_apparel_orders_data\"\nitems_dir  = f\"{root}/ml_ozon_recsys_train_final_apparel_items_data/ml_ozon_recsys_train_final_apparel_items_data\"\n\nprint(\"Собираем списки партов...\")\ntest_files   = find_parts(test_path)\ncatal_files  = find_parts(catal_dir)\ntracker_files= find_parts(tracker_dir)\norders_files = find_parts(orders_dir)\nitems_files  = find_parts(items_dir)\nprint(f\"Найдено файлов: items={len(items_files)}, tracker={len(tracker_files)}, orders={len(orders_files)}\")\n\nprint(\"Чтение test...\")\ntest_df = pl.concat([pl.read_parquet(f) for f in test_files]) if len(test_files) > 1 else pl.read_parquet(test_files[0])\nif \"user_id\" in test_df.columns:\n    test_df = test_df.unique(subset=[\"user_id\"], keep=\"last\")\ntest_users = test_df[\"user_id\"].unique().to_list()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 3) max ts\n# -----------------------\ndef _detect_epoch_unit(sample: pl.Series) -> str:\n    # Очень грубая, но рабочая эвристика по порядку величины\n    if len(sample) == 0:\n        return \"ms\"\n    mx = int(pl.Series(sample).drop_nulls().max())  # защита от пустоты\n    if mx > 10**14:\n        return \"ns\"\n    elif mx > 10**12:\n        return \"us\"\n    elif mx > 10**10:\n        return \"ms\"\n    else:\n        return \"s\"\n\ndef robust_max_ts_from_files(files: List[str], ts_candidates: Tuple[str, ...]) -> Optional[datetime]:\n    if not files:\n        return None\n    lf = pl.scan_parquet(files)\n    schema = lf.schema\n\n    # ищем реальную колонку времени\n    chosen = None\n    for c in ts_candidates:\n        if c in schema:\n            chosen = c\n            break\n    if chosen is None:\n        print(f\"[robust_max_ts] Нет подходящих колонок времени среди {ts_candidates}. Доступны: {list(schema.keys())[:20]}\")\n        return None\n\n    dtype = schema[chosen]\n    expr = pl.col(chosen)\n\n    if dtype == pl.Utf8:\n        # строковый ISO -> Datetime\n        expr = expr.str.strptime(pl.Datetime, strict=False)\n    elif dtype in (pl.Int64, pl.Int32, pl.UInt64, pl.UInt32, pl.Int16, pl.UInt16, pl.Int8, pl.UInt8):\n        # epoch -> Datetime (определяем unit по сэмплу из первого файла)\n        try:\n            sample = pl.read_parquet(files[0], columns=[chosen], n_rows=500)[chosen]\n        except Exception:\n            sample = pl.Series([], dtype=pl.Int64)\n        unit = _detect_epoch_unit(sample)\n        expr = pl.from_epoch(expr, time_unit=unit)\n    # если уже Datetime — оставляем как есть\n\n    out = (\n        lf.select(expr.drop_nulls().max().alias(\"max_ts\"))\n          .collect(streaming=True)\n    )\n    return out[\"max_ts\"][0]\n\n# Вызываем так (добавили кандидаты на названия колонок)\nt1 = robust_max_ts_from_files(tracker_files, (\"timestamp\", \"event_time\", \"event_timestamp\"))\nt2 = robust_max_ts_from_files(orders_files,  (\"created_timestamp\", \"created_at\", \"timestamp\", \"event_time\", \"event_timestamp\"))\n\nif t1 is None and t2 is None:\n    # Диагностика, чтобы понять, какие вообще есть поля\n    print(\"Схема tracker:\", pl.scan_parquet(tracker_files).schema if tracker_files else {})\n    print(\"Схема orders:\", pl.scan_parquet(orders_files).schema if orders_files else {})\n    raise RuntimeError(\"Не удалось определить max_ts — не нашли колонок времени или все пустые\")\n\nmax_ts = t1 if t2 is None else (t2 if t1 is None else max(t1, t2))\nT = max_ts - timedelta(days=VAL_DAYS)\nTRAIN_FROM = T - timedelta(days=TRAIN_WINDOW_DAYS)\nprint(f\"Robust max timestamp: {max_ts}, Train/Val split T: {T}, Train from: {TRAIN_FROM}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------------------------\n# 3.5) Популярные товары\n# -----------------------------------------------\ndef get_popular_items_from_orders(order_files: List[str], min_deliveries: int) -> Set[int]:\n    print(f\"Определяем популярные товары (минимум {min_deliveries} доставок)...\")\n    if not order_files:\n        warnings.warn(\"Файлы заказов не найдены.\")\n        return set()\n    scan = pl.scan_parquet(order_files)\n    popular_items_df = (\n        scan.filter(pl.col(\"last_status\") == \"delivered_orders\")\n            .group_by(\"item_id\")\n            .agg(pl.len().alias(\"delivery_count\"))\n            .filter(pl.col(\"delivery_count\") >= min_deliveries)\n            .select(\"item_id\")\n            .collect(streaming=True)\n    )\n    popular_set = set(popular_items_df[\"item_id\"].to_list())\n    print(f\"Найдено {len(popular_set)} популярных товаров.\")\n    return popular_set\n\npopular_items_set = get_popular_items_from_orders(orders_files, MIN_DELIVERED_ORDERS)\nif not popular_items_set:\n    raise RuntimeError(\"Не удалось сформировать список популярных товаров.\")\n\n# ----------------------------------------------------\n# 4) Категории (id->cat_l0)\n# ----------------------------------------------------\ndef extract_cat_levels(ids_list: Optional[List[int]], n_levels: int = 4) -> List[Optional[int]]:\n    if ids_list is None: return [None] * n_levels\n    clean = [x for x in ids_list if x != -1]\n    out = clean + [None] * max(0, n_levels - len(clean))\n    return out[:n_levels]\n\ncatalog_to_cat0: Dict[int, Optional[int]] = {}\nprint(\"Строим mapping catalogid -> cat_l0 ...\")\nfor f in catal_files:\n    df = pl.read_parquet(f, columns=[\"catalogid\",\"ids\"])\n    if df.is_empty(): continue\n    df = df.unique(subset=[\"catalogid\"], keep=\"last\")\n    cat_lvls = [extract_cat_levels(x) for x in df[\"ids\"].to_list()]\n    cat0 = [lv[0] if len(lv) > 0 else None for lv in cat_lvls]\n    for cid, c0 in zip(df[\"catalogid\"].to_list(), cat0):\n        catalog_to_cat0[int(cid)] = c0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 5) События одним ленивым пайплайном (NEW)\n# -----------------------\ndef _make_ts_expr(lf: pl.LazyFrame, col_name: str, files: List[str]) -> pl.Expr:\n    schema = lf.schema\n    if col_name not in schema:\n        raise pl.ColumnNotFoundError(f\"Колонка {col_name} не найдена\")\n    dtype = schema[col_name]\n    expr = pl.col(col_name)\n    if dtype == pl.Utf8:\n        expr = expr.str.strptime(pl.Datetime, strict=False)\n    elif dtype in (pl.Int64, pl.Int32, pl.UInt64, pl.UInt32, pl.Int16, pl.UInt16, pl.Int8, pl.UInt8):\n        try:\n            sample = pl.read_parquet(files[0], columns=[col_name], n_rows=500)[col_name]\n        except Exception:\n            sample = pl.Series([], dtype=pl.Int64)\n        unit = _detect_epoch_unit(sample)\n        expr = pl.from_epoch(expr, time_unit=unit)\n    return expr\n\ndef build_events_tables(tracker_files, orders_files, item_filter: Set[int]):\n    decay = (-math.log(2.0)/30.0)\n\n    # tracker\n    ltr = pl.scan_parquet(tracker_files).select([\"user_id\",\"item_id\",\"action_type\",\"timestamp\"])\n    tr_ts = _make_ts_expr(ltr, \"timestamp\", tracker_files)\n    tr = (\n        ltr\n        .with_columns(tr_ts.alias(\"timestamp\"))\n        .filter(pl.col(\"item_id\").is_in(item_filter))\n        .filter(pl.col(\"timestamp\") >= TRAIN_FROM)\n        .with_columns(\n            pl.when(pl.col(\"action_type\") == \"page_view\").then(pl.lit(track_w[\"page_view\"]))\n            .when(pl.col(\"action_type\") == \"view_description\").then(pl.lit(track_w[\"view_description\"]))\n            .when(pl.col(\"action_type\") == \"to_cart\").then(pl.lit(track_w[\"to_cart\"]))\n            .when(pl.col(\"action_type\") == \"favorite\").then(pl.lit(track_w[\"favorite\"]))\n            .when(pl.col(\"action_type\") == \"review_view\").then(pl.lit(track_w[\"review_view\"]))\n            .otherwise(pl.lit(1.0)).alias(\"weight\")\n        )\n        .with_columns(\n            (pl.col(\"weight\") * (decay * (pl.lit(max_ts) - pl.col(\"timestamp\")).dt.total_days().clip(0, None)).exp())\n            .alias(\"strength\")\n        )\n        .select([\"user_id\",\"item_id\",\"timestamp\",\"strength\"])\n    )\n\n    # orders\n    lod = pl.scan_parquet(orders_files).select(\n        [pl.col(\"user_id\"), pl.col(\"item_id\"), pl.col(\"created_timestamp\").alias(\"timestamp\"), pl.col(\"last_status\")]\n    )\n    od_ts = _make_ts_expr(lod, \"timestamp\", orders_files)\n    od = (\n        lod\n        .with_columns(od_ts.alias(\"timestamp\"))\n        .filter(pl.col(\"item_id\").is_in(item_filter))\n        .filter(pl.col(\"timestamp\") >= TRAIN_FROM)\n        .with_columns(\n            pl.when(pl.col(\"last_status\") == \"delivered_orders\").then(pl.lit(orders_w[\"delivered_orders\"]))\n            .when(pl.col(\"last_status\") == \"proccesed_orders\").then(pl.lit(orders_w[\"proccesed_orders\"]))\n            .when(pl.col(\"last_status\") == \"canceled_orders\").then(pl.lit(orders_w[\"canceled_orders\"]))\n            .otherwise(pl.lit(1.0)).alias(\"weight\")\n        )\n        .with_columns(\n            (pl.col(\"weight\") * (decay * (pl.lit(max_ts) - pl.col(\"timestamp\")).dt.total_days().clip(0, None)).exp())\n            .alias(\"strength\")\n        )\n        .select([\"user_id\",\"item_id\",\"timestamp\",\"strength\"])\n    )\n\n    ev = pl.concat([tr, od])\n\n    ui_train = (\n        ev.filter(pl.col(\"timestamp\") < pl.lit(T))\n          .group_by([\"user_id\",\"item_id\"])\n          .agg(pl.col(\"strength\").sum().alias(\"ui_strength\"))\n          .collect(streaming=True)\n    )\n\n    ui_val = (\n        ev.filter(pl.col(\"timestamp\") >= pl.lit(T))\n          .group_by([\"user_id\",\"item_id\"])\n          .agg(pl.col(\"strength\").sum().alias(\"label\"))\n          .with_columns(pl.col(\"label\").clip(0, 1.0))\n          .collect(streaming=True)\n    )\n\n    item_pop_df = (\n        ev.group_by(\"item_id\")\n          .agg(pl.col(\"strength\").sum().alias(\"item_pop\"))\n          .collect(streaming=True)\n    )\n\n    return ui_train, ui_val, item_pop_df\n\nprint(\"Строим финальные ui_train/ui_val/pop (ленивый пайплайн)...\")\nui_train, ui_val, item_pop_df = build_events_tables(tracker_files, orders_files, popular_items_set)\n\nitem_to_pop: Dict[int, float] = dict(zip(item_pop_df[\"item_id\"].to_list(), item_pop_df[\"item_pop\"].to_list()))\ntop_pop = item_pop_df.sort(\"item_pop\", descending=True).head(400)[\"item_id\"].to_list()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 6) ALS на всем ui_train\n# -----------------------\ndef train_als(ui_train: pl.DataFrame):\n    unique_users = ui_train[\"user_id\"].unique().sort().to_list()\n    unique_items = ui_train[\"item_id\"].unique().sort().to_list()\n    uid_to_uix = {u: i for i, u in enumerate(unique_users)}\n    iid_to_iix = {i: j for j, i in enumerate(unique_items)}\n    iix_to_itemid = unique_items\n\n    user_map_df = pl.DataFrame({\"user_id\": list(uid_to_uix.keys()), \"uix\": list(uid_to_uix.values())}).with_columns(pl.col(\"user_id\").cast(ui_train[\"user_id\"].dtype))\n    item_map_df = pl.DataFrame({\"item_id\": list(iid_to_iix.keys()), \"iix\": list(iid_to_iix.values())}).with_columns(pl.col(\"item_id\").cast(ui_train[\"item_id\"].dtype))\n\n    ui_np = (\n        ui_train.join(user_map_df, on=\"user_id\", how=\"left\")\n                .join(item_map_df, on=\"item_id\", how=\"left\")\n                .fill_null(-1)\n                .filter((pl.col(\"uix\") >= 0) & (pl.col(\"iix\") >= 0))\n    )\n\n    data = ui_np[\"ui_strength\"].cast(pl.Float32).to_numpy()\n    rows = ui_np[\"uix\"].cast(pl.Int32).to_numpy()\n    cols = ui_np[\"iix\"].cast(pl.Int32).to_numpy()\n\n    n_users, n_items = len(uid_to_uix), len(iid_to_iix)\n    user_items_csr = sp.csr_matrix((data, (rows, cols)), shape=(n_users, n_items), dtype=np.float32)\n\n    item_users_bm25 = bm25_weight(user_items_csr.T, K1=100, B=0.8).astype(np.float32)\n\n    als_model = AlternatingLeastSquares(\n        factors=ALS_FACTORS, iterations=ALS_ITERS, regularization=ALS_REG,\n        random_state=SEED, num_threads=NUM_THREADS\n    )\n    als_model.fit(item_users_bm25)\n\n    return dict(\n        als_model=als_model,\n        user_items_csr=user_items_csr,\n        uid_to_uix=uid_to_uix,\n        iid_to_iix=iid_to_iix,\n        iix_to_itemid=iix_to_itemid,\n    )\n\nprint(\"Обучаем ALS...\")\nals_bundle = train_als(ui_train)\nals_model = als_bundle[\"als_model\"]\nuser_items_csr = als_bundle[\"user_items_csr\"]\nuid_to_uix      = als_bundle[\"uid_to_uix\"]\niid_to_iix      = als_bundle[\"iid_to_iix\"]\niix_to_itemid   = als_bundle[\"iix_to_itemid\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 7) I2I on-demand через BM25Recommender (NEW, заменяет co-vis)\n# -----------------------\nclass I2IOnDemand:\n    def __init__(self, item_users: sp.csr_matrix, topk: int, iix_to_itemid: List[int]):\n        self.topk = topk\n        self.model = BM25Recommender(K=topk, num_threads=NUM_THREADS)\n        self.model.fit(item_users)  # item_users: items x users CSR\n        self.iix_to_itemid = iix_to_itemid\n        self._cache = OrderedDict()\n        self._lock = threading.Lock()\n        self._capacity = 200000  # LRU для соседей\n\n    def neighbors(self, item_id: int, iid_to_iix: Dict[int, int]):\n        iix = iid_to_iix.get(item_id)\n        if iix is None:\n            return np.empty(0, dtype=np.int64), np.empty(0, dtype=np.float32)\n\n        with self._lock:\n            if iix in self._cache:\n                self._cache.move_to_end(iix)\n                return self._cache[iix]\n\n        ids, scores = self.model.similar_items(iix, N=self.topk + 1)\n        nbrs, scs = [], []\n        for j, s in zip(ids, scores):\n            if int(j) == iix:\n                continue\n            if 0 <= int(j) < len(self.iix_to_itemid):\n                nbrs.append(self.iix_to_itemid[int(j)])\n                scs.append(float(s))\n        nbrs = np.array(nbrs[:self.topk], dtype=np.int64)\n        scs = np.array(scs[:self.topk], dtype=np.float32)\n\n        with self._lock:\n            self._cache[iix] = (nbrs, scs)\n            if len(self._cache) > self._capacity:\n                self._cache.popitem(last=False)\n        return nbrs, scs\n\ni2i_model = I2IOnDemand(user_items_csr.T.tocsr(), topk=120, iix_to_itemid=iix_to_itemid)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 8) Эмбеддинги и категории: строим ТОЛЬКО для используемых items (NEW)\n# -----------------------\nitem_to_cat: Dict[int, Optional[int]] = {}\nitemid_to_embix: Dict[int, int] = {}\nembix_to_itemid_cache: Dict[int, int] = {}\nemb_idx = None\nemb_dim = None\nannoy_next_ix = 0\n\n# Таргетные товары: из train + top_pop\ntarget_items = set(ui_train[\"item_id\"].unique().to_list()) | set(top_pop)\ntarget_items = list(target_items)\n\nprint(f\"Строим cat map и Annoy только для {len(target_items)} items...\")\ntarget_set = set(target_items)\n\nfor f in items_files:\n    try:\n        df = pl.read_parquet(f, columns=[\"item_id\",\"catalogid\",\"fclip_embed\"])\n    except pl.ColumnNotFoundError:\n        df = pl.read_parquet(f, columns=[\"item_id\",\"catalogid\"])\n    if df.is_empty():\n        continue\n    df = df.filter(pl.col(\"item_id\").is_in(target_set))\n    if df.is_empty():\n        continue\n    df = df.rename({\"fclip_embed\": \"embed\"})\n\n    # cat\n    for it, cid in zip(df[\"item_id\"].to_list(), df[\"catalogid\"].to_list()):\n        item_to_cat[int(it)] = catalog_to_cat0.get(int(cid), None)\n\n    # embed\n    if \"embed\" in df.columns:\n        for it, emb in zip(df[\"item_id\"].to_list(), df[\"embed\"].to_list()):\n            if emb is None: \n                continue\n            vec = np.asarray(emb, dtype=np.float32)\n            if emb_dim is None:\n                emb_dim = len(vec)\n                emb_idx = AnnoyIndex(emb_dim, \"angular\")\n            if len(vec) != emb_dim:\n                continue\n            emb_idx.add_item(annoy_next_ix, vec.tolist())\n            itemid_to_embix[int(it)] = annoy_next_ix\n            embix_to_itemid_cache[annoy_next_ix] = int(it)\n            annoy_next_ix += 1\n    del df; gc.collect()\n\nif emb_idx is not None and annoy_next_ix > 0:\n    emb_idx.build(ANNOY_TREES)\nelse:\n    print(\"Annoy: эмбеддинги не найдены или пусты.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 9) Вспомогательные ф-и и кандидаты\n# -----------------------\ndef normalize_scores(pairs: List[Tuple[int, float]]) -> Dict[int, float]:\n    if not pairs:\n        return {}\n    s = np.array([v for _, v in pairs], dtype=np.float32)\n    s_min, s_max = float(s.min()), float(s.max())\n    denom = (s_max - s_min) if s_max > s_min else 1.0\n    return {int(it): float((v - s_min) / denom) for it, v in pairs}\n\ndef get_user_row(u: int):\n    uix = uid_to_uix.get(u)\n    if uix is None:\n        return None\n    return user_items_csr.getrow(uix)\n\ndef topk_hist_from_row(row, k=50):\n    if row is None or row.nnz == 0:\n        return []\n    idx = row.indices\n    vals = row.data\n    if len(vals) <= k:\n        order = np.argsort(-vals)\n    else:\n        top_idx = np.argpartition(-vals, k-1)[:k]\n        order = top_idx[np.argsort(-vals[top_idx])]\n    return [(iix_to_itemid[int(idx[j])], float(vals[j])) for j in order]\n\ndef get_seen_from_row(row):\n    if row is None or row.nnz == 0:\n        return set()\n    return set([iix_to_itemid[int(i)] for i in row.indices])\n\ndef get_pref_cats_from_row(row, k=200):\n    hist = topk_hist_from_row(row, k=k)\n    agg: Dict[int, float] = {}\n    for it, w in hist:\n        c0 = item_to_cat.get(int(it))\n        if c0 is None: continue\n        agg[c0] = agg.get(c0, 0.0) + float(w)\n    if not agg:\n        return set()\n    cats = list(agg.keys())\n    vals = np.array(list(agg.values()), dtype=np.float32)\n    order = np.argsort(-vals)[:3]\n    return set([int(cats[i]) for i in order])\n\ndef build_user_profile_from_row(row, use_k=50):\n    if emb_idx is None:\n        return None\n    hist = topk_hist_from_row(row, k=use_k)\n    vec = None\n    denom = 0.0\n    for it, w in hist:\n        eix = itemid_to_embix.get(int(it))\n        if eix is None:\n            continue\n        v = np.asarray(emb_idx.get_item_vector(eix), dtype=np.float32)\n        vec = v * float(w) if vec is None else (vec + v * float(w))\n        denom += float(w)\n    if vec is None or denom <= 0:\n        return None\n    vec = vec / (np.linalg.norm(vec) + 1e-12)\n    return vec.astype(np.float32)\n\ndef i2i_candidates_cov(u: int, topn: int = I2I_TOPN, recent_k: int = RECENT_K_FOR_I2I):\n    row = get_user_row(u)\n    hist = topk_hist_from_row(row, k=recent_k)\n    if not hist:\n        return []\n    scores: Dict[int, float] = {}\n    for it, w in hist:\n        nbrs, scs = i2i_model.neighbors(int(it), iid_to_iix)\n        for nb, s in zip(nbrs, scs):\n            scores[int(nb)] = scores.get(int(nb), 0.0) + float(s) * float(w)\n    if not scores:\n        return []\n    items = np.fromiter(scores.keys(), dtype=np.int64)\n    vals = np.fromiter(scores.values(), dtype=np.float32)\n    k = min(topn, len(vals))\n    idx = np.argpartition(-vals, k-1)[:k]\n    idx = idx[np.argsort(-vals[idx])]\n    return [(int(items[i]), float(vals[i])) for i in idx]\n\ndef als_candidates(u: int, topn: int = ALS_TOPN):\n    uix = uid_to_uix.get(u)\n    if uix is None:\n        return []\n    try:\n        ids, scores = als_model.recommend(\n            userid=uix, user_items=user_items_csr[uix], N=topn,\n            filter_already_liked_items=True, recalculate_user=False\n        )\n    except Exception as e:\n        warnings.warn(f\"ALS recommend failed for user {u}: {e}\")\n        return []\n    out = []\n    for i, s in zip(ids, scores):\n        if 0 <= int(i) < len(iix_to_itemid):\n            out.append((iix_to_itemid[int(i)], float(s)))\n    return out\n\ndef embed_knn(u: int, topn: int = EMB_TOPN):\n    if emb_idx is None:\n        return []\n    row = get_user_row(u)\n    prof = build_user_profile_from_row(row)\n    if prof is None:\n        return []\n    try:\n        idxs, dists = emb_idx.get_nns_by_vector(prof.tolist(), n=topn, include_distances=True)\n    except Exception as e:\n        warnings.warn(f\"Annoy KNN failed for user {u}: {e}\")\n        return []\n    out = []\n    for ix in idxs:\n        it = embix_to_itemid_cache.get(ix)\n        if it is None:\n            continue\n        v = np.asarray(emb_idx.get_item_vector(ix), dtype=np.float32)\n        s = float(v @ prof)\n        out.append((int(it), s))\n    return out\n\ndef blend_user(u: int, topk: int = BLEND_TOPK):\n    c_cov = i2i_candidates_cov(u, topn=I2I_TOPN, recent_k=RECENT_K_FOR_I2I)\n    c_als = als_candidates(u, topn=ALS_TOPN)\n    c_emb = embed_knn(u, topn=EMB_TOPN)\n\n    s_cov = normalize_scores(c_cov)\n    s_als = normalize_scores(c_als)\n    s_emb = normalize_scores(c_emb)\n\n    w_cov, w_als, w_emb = 0.6, 0.25, 0.15\n    all_items = set(s_cov) | set(s_als) | set(s_emb)\n\n    row = get_user_row(u)\n    seen = get_seen_from_row(row)\n    pref_cats = get_pref_cats_from_row(row, k=200)\n\n    scores = []\n    for it in all_items:\n        if it in seen:\n            continue\n        sc = w_cov * s_cov.get(it, 0.0) + w_als * s_als.get(it, 0.0) + w_emb * s_emb.get(it, 0.0)\n        c0 = item_to_cat.get(int(it))\n        if c0 is not None and c0 in pref_cats:\n            sc += 0.03\n        pop = item_to_pop.get(int(it), 0.0)\n        sc += math.log1p(pop) * 1e-3\n        scores.append((int(it), float(sc), float(s_cov.get(it, 0.0)), float(s_als.get(it, 0.0)), float(s_emb.get(it, 0.0))))\n\n    scores.sort(key=lambda x: x[1], reverse=True)\n    \n    if len(scores) < topk:\n        existing_items = {s[0] for s in scores} | seen\n        pop_candidates = []\n        \n        for it in top_pop:\n            if it not in existing_items:\n                pop_candidates.append((int(it), 0.0, 0.0, 0.0, 0.0))\n                if len(scores) + len(pop_candidates) >= topk:\n                    break\n        \n        if len(scores) + len(pop_candidates) < topk:\n            all_pop_items = list(item_to_pop.keys())\n            all_pop_items.sort(key=lambda x: item_to_pop[x], reverse=True)\n            for it in all_pop_items:\n                if it not in existing_items and it not in {p[0] for p in pop_candidates}:\n                    pop_candidates.append((int(it), 0.0, 0.0, 0.0, 0.0))\n                    if len(scores) + len(pop_candidates) >= topk:\n                        break\n        \n        scores.extend(pop_candidates)\n    \n    return scores[:topk]\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 10) Генерация рекомендаций батчами\n# -----------------------\ndef generate_recs_for_users(users: List[int], batch_size: int = 4000, max_workers: Optional[int] = None) -> pl.DataFrame:\n    all_dfs = []\n    total_users = len(users)\n    print(f\"Генерируем рекомендации для {total_users} пользователей...\")\n    \n    for i in range(0, len(users), batch_size):\n        batch = users[i:i+batch_size]\n        rows = []\n        workers = max_workers or min(8, CPU_COUNT // 2 or 1)\n        \n        with ThreadPoolExecutor(max_workers=workers) as ex:\n            futs = {ex.submit(blend_user, u, BLEND_TOPK): u for u in batch}\n            for fut in as_completed(futs):\n                u = futs[fut]\n                try:\n                    recs = fut.result()\n                    if len(recs) < BLEND_TOPK:\n                        print(f\"Предупреждение: пользователь {u} получил только {len(recs)} рекомендаций\")\n                    for rank, (it, sc, cov_s, als_s, emb_s) in enumerate(recs, 1):\n                        rows.append((u, int(it), int(rank), float(sc), float(cov_s), float(als_s), float(emb_s)))\n                except Exception as e:\n                    warnings.warn(f\"Failed to generate recs for user {u}: {e}\")\n                    for rank, it in enumerate(top_pop[:BLEND_TOPK], 1):\n                        rows.append((u, int(it), int(rank), 0.0, 0.0, 0.0, 0.0))\n        \n        print(f\"Обработано {min(i+batch_size, total_users)}/{total_users} пользователей\")\n        dfb = pl.DataFrame(rows, schema=[\"user_id\", \"item_id\", \"rank\", \"cand_score\", \"i2i_s\", \"als_s\", \"clip_s\"])\n        all_dfs.append(dfb)\n        del rows; gc.collect()\n    \n    result = pl.concat(all_dfs) if all_dfs else pl.DataFrame(schema=[\"user_id\",\"item_id\",\"rank\",\"cand_score\",\"i2i_s\",\"als_s\",\"clip_s\"])\n    \n    user_counts = result.group_by(\"user_id\").agg(pl.len().alias(\"count\"))\n    print(f\"Статистика количества кандидатов на пользователя до обработки:\")\n    print(user_counts[\"count\"].describe())\n    \n    return result\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 11) Лёгкие фичи для реранка\n# -----------------------\ndef build_features(cand_df: pl.DataFrame) -> pl.DataFrame:\n    if cand_df.is_empty():\n        return cand_df\n    parts = cand_df.partition_by(\"user_id\", as_dict=True)\n    out = []\n    for key, g in parts.items():\n        u = int(key[0] if isinstance(key, tuple) else key)\n        row = get_user_row(u)\n        prof = build_user_profile_from_row(row)\n        pref_cats = get_pref_cats_from_row(row, k=200)\n\n        if prof is not None and emb_idx is not None:\n            sims = []\n            for it in g[\"item_id\"].to_list():\n                eix = itemid_to_embix.get(int(it))\n                if eix is None:\n                    sims.append(0.0)\n                else:\n                    try:\n                        v = np.asarray(emb_idx.get_item_vector(eix), dtype=np.float32)\n                        sims.append(float(v @ prof))\n                    except:\n                        sims.append(0.0)\n            g = g.with_columns(pl.Series(\"sim_up_item\", np.array(sims, dtype=np.float32)))\n        else:\n            g = g.with_columns(pl.lit(0.0).cast(pl.Float32).alias(\"sim_up_item\"))\n\n        pops = [float(item_to_pop.get(int(it), 0.0)) for it in g[\"item_id\"].to_list()]\n        flags = []\n        for it in g[\"item_id\"].to_list():\n            c0 = item_to_cat.get(int(it))\n            flags.append(1 if (c0 is not None and c0 in pref_cats) else 0)\n        g = g.with_columns([\n            pl.Series(\"item_pop\", np.array(pops, dtype=np.float32)),\n            pl.Series(\"same_cat_l0\", np.array(flags, dtype=np.int8)),\n        ])\n        out.append(g)\n    df = pl.concat(out) if out else cand_df\n    for col in [\"i2i_s\", \"als_s\", \"clip_s\", \"sim_up_item\", \"item_pop\"]:\n        if col in df.columns:\n            df = df.with_columns(pl.col(col).cast(pl.Float32))\n    if \"same_cat_l0\" in df.columns:\n        df = df.with_columns(pl.col(\"same_cat_l0\").cast(pl.Int8))\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 12) Реранк\n# -----------------------\npositives_by_user: Dict[int, set] = {}\nfor r in ui_val.iter_rows(named=True):\n    positives_by_user.setdefault(int(r[\"user_id\"]), set()).add(int(r[\"item_id\"]))\n\nval_users = list(set(ui_val[\"user_id\"].unique().to_list()))\nif MAX_VAL_USERS and len(val_users) > MAX_VAL_USERS:\n    rng = np.random.default_rng(SEED)\n    val_users = rng.choice(val_users, size=MAX_VAL_USERS, replace=False).tolist()\n\nrerank_train_rows = []\nfor u in val_users[:400]:\n    try:\n        cands = blend_user(u, topk=TRAIN_CANDS_PER_USER)\n    except Exception as e:\n        warnings.warn(f\"Failed to generate training candidates for user {u}: {e}\")\n        continue\n    if not cands:\n        continue\n    pos = positives_by_user.get(u, set())\n    base = [(u, it, cov_s, als_s, emb_s) for it, _, cov_s, als_s, emb_s in cands]\n    dfb = pl.DataFrame(base, schema=[\"user_id\", \"item_id\", \"i2i_s\", \"als_s\", \"clip_s\"])\n    g = pl.DataFrame([{\"user_id\": u, \"item_id\": int(it), \"label\": 1.0 if it in pos else 0.0} for it in (set(dfb[\"item_id\"].to_list()) | pos)])\n    g = g.join(dfb, on=[\"user_id\", \"item_id\"], how=\"left\").fill_null(0.0)\n    rerank_train_rows.append(g)\n\nrerank_train_df = pl.concat(rerank_train_rows) if rerank_train_rows else pl.DataFrame({\"user_id\": [], \"item_id\": [], \"label\": [], \"i2i_s\": [], \"als_s\": [], \"clip_s\": []})\n\nranker = None\nif CATBOOST_AVAILABLE and not rerank_train_df.is_empty():\n    try:\n        train_feat = build_features(rerank_train_df)\n        features = [\"i2i_s\", \"als_s\", \"clip_s\", \"sim_up_item\", \"item_pop\", \"same_cat_l0\"]\n        all_train_users = train_feat[\"user_id\"].unique().to_list()\n        if len(all_train_users) > 1:\n            train_users_idx, eval_users_idx = next(\n                GroupShuffleSplit(test_size=0.1, n_splits=1, random_state=SEED).split(all_train_users, groups=all_train_users)\n            )\n            train_users = {all_train_users[i] for i in train_users_idx}\n            eval_users = {all_train_users[i] for i in eval_users_idx}\n\n            train_part = train_feat.filter(pl.col(\"user_id\").is_in(train_users))\n            eval_part = train_feat.filter(pl.col(\"user_id\").is_in(eval_users))\n            if not train_part.is_empty() and not eval_part.is_empty():\n                train_pool = Pool(data=train_part.select(features).to_pandas(), label=train_part[\"label\"].to_pandas(), group_id=train_part[\"user_id\"].to_pandas())\n                eval_pool = Pool(data=eval_part.select(features).to_pandas(), label=eval_part[\"label\"].to_pandas(), group_id=eval_part[\"user_id\"].to_pandas())\n\n                task_type = \"GPU\" if os.environ.get(\"CUDA_VISIBLE_DEVICES\") else \"CPU\"\n                ranker = CatBoostRanker(\n                    iterations=CB_ITER, depth=CB_DEPTH, learning_rate=CB_LR,\n                    loss_function=\"YetiRank\", random_seed=SEED,\n                    task_type=task_type, devices='0' if task_type == \"GPU\" else None,\n                    thread_count=NUM_THREADS, od_type=\"Iter\", od_wait=CB_OD_WAIT, verbose=100\n                )\n                ranker.fit(train_pool, eval_set=eval_pool, use_best_model=True)\n    except Exception as e:\n        warnings.warn(f\"CatBoost training failed: {e}\")\n        ranker = None\nelse:\n    print(\"CatBoost недоступен/нет данных — используем blended score.\")\n\nprint(\"Генерируем рекомендации для теста...\")\ntest_cand_df = generate_recs_for_users(test_users, batch_size=4000)\n\n# Проверяем сколько кандидатов сгенерировано\nprint(f\"Всего сгенерировано строк: {len(test_cand_df)}\")\nprint(f\"Уникальных пользователей: {test_cand_df['user_id'].n_unique()}\")\n\ntest_feat = build_features(test_cand_df.select([\"user_id\", \"item_id\", \"i2i_s\", \"als_s\", \"clip_s\"]))\ntest_feat = test_feat.join(test_cand_df, on=[\"user_id\", \"item_id\"], how=\"left\")\n\nif CATBOOST_AVAILABLE and ranker is not None and not test_feat.is_empty():\n    print(\"Реранк CatBoost...\")\n    features = [\"i2i_s\", \"als_s\", \"clip_s\", \"sim_up_item\", \"item_pop\", \"same_cat_l0\"]\n    preds = ranker.predict(Pool(data=test_feat.select(features).to_pandas()))\n    test_feat = test_feat.with_columns(pl.Series(name=\"rerank_score\", values=preds))\n    recs_df = (\n        test_feat\n        .with_columns(pl.col(\"rerank_score\").alias(\"score\"))\n        .sort([\"user_id\", \"score\"], descending=[False, True])\n        .group_by(\"user_id\", maintain_order=True)\n        .head(100)\n        .select([\"user_id\", \"item_id\"])\n    )\nelse:\n    print(\"Используем blended score.\")\n    recs_df = (\n        test_feat\n        .with_columns(pl.col(\"cand_score\").alias(\"score\"))\n        .sort([\"user_id\", \"score\"], descending=[False, True])\n        .group_by(\"user_id\", maintain_order=True)\n        .head(100)\n        .select([\"user_id\", \"item_id\"])\n    )\n\nuser_rec_counts = recs_df.group_by(\"user_id\").agg(pl.len().alias(\"rec_count\"))\nprint(f\"Статистика количества рекомендаций на пользователя после ранжирования:\")\nprint(user_rec_counts[\"rec_count\"].describe())\n\nusers_needing_more = user_rec_counts.filter(pl.col(\"rec_count\") < 100)\nif len(users_needing_more) > 0:\n    print(f\"Дополняем рекомендации для {len(users_needing_more)} пользователей...\")\n    additional_recs = []\n    \n    for row in users_needing_more.iter_rows(named=True):\n        u = row[\"user_id\"]\n        current_count = row[\"rec_count\"]\n        needed = 100 - current_count\n        \n        existing = set(recs_df.filter(pl.col(\"user_id\") == u)[\"item_id\"].to_list())\n        \n        added = 0\n        for it in top_pop:\n            if it not in existing:\n                additional_recs.append({\"user_id\": u, \"item_id\": it})\n                existing.add(it)\n                added += 1\n                if added >= needed:\n                    break\n        \n        if added < needed:\n            all_items = list(item_to_pop.keys())\n            all_items.sort(key=lambda x: item_to_pop[x], reverse=True)\n            for it in all_items:\n                if it not in existing:\n                    additional_recs.append({\"user_id\": u, \"item_id\": it})\n                    added += 1\n                    if added >= needed:\n                        break\n    \n    if additional_recs:\n        additional_df = pl.DataFrame(additional_recs)\n        recs_df = pl.concat([recs_df, additional_df])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Финальная проверка\nfinal_counts = recs_df.group_by(\"user_id\").agg(pl.len().alias(\"rec_count\"))\nprint(f\"\\nФинальная статистика количества рекомендаций:\")\nprint(f\"Минимум: {final_counts['rec_count'].min()}\")\nprint(f\"Максимум: {final_counts['rec_count'].max()}\")\nprint(f\"Среднее: {final_counts['rec_count'].mean():.2f}\")\n\n# Сохраняем\nrecs_df.write_csv(\"contribution.csv\")\nprint(f\"contribution.csv сохранён. Размер: {len(recs_df)} строк\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
